{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8607b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c19df",
   "metadata": {},
   "source": [
    "# Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9154de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"data/manual_eval/reports_txt\"\n",
    "\n",
    "# Get sorted list of filenames\n",
    "file_list = sorted([f for f in os.listdir(folder_path) if f.endswith(\".txt\")])\n",
    "\n",
    "# Load each file's text into a list\n",
    "documents = []\n",
    "for filename in file_list:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "\n",
    "# Optional: print a preview\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"Document {i+1} preview:\\n{doc[:100]}...\\n\")\n",
    "\n",
    "file_list\n",
    "documents\n",
    "print(documents[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdf0b9",
   "metadata": {},
   "source": [
    "# Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533323e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initializes a CountVectorizer object - will learn a vocabulary from text and convert document(s)\n",
    "# into a vector of word counts\n",
    "BoW_vectorizer = CountVectorizer()\n",
    "# Builds vocabulary of all unique words found across document(s)\n",
    "# Converts document(s) into sparse vector indicating how many times each word\n",
    "# from the vocabulary appears\n",
    "BoW_X = BoW_vectorizer.fit_transform(documents)\n",
    "\n",
    "features = BoW_vectorizer.get_feature_names_out()  # List of words in vocabulary\n",
    "#print(BoW_X.toarray()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features[1000:1100])\n",
    "BoW_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018abee0",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing reduces explained variance in first components\n",
    "# scaler = StandardScaler(with_mean=False)\n",
    "# BoW_X = scaler.fit_transform(X)\n",
    "BoW_X = BoW_X.toarray()\n",
    "BoW_pca = PCA(n_components=2)\n",
    "BoW_pca.fit(BoW_X)\n",
    "BoW_pca_X = BoW_pca.transform(BoW_X)\n",
    "\n",
    "print(BoW_pca.explained_variance_ratio_)\n",
    "print(BoW_pca.singular_values_)\n",
    "print(sum(BoW_pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a14b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW_X.shape\n",
    "BoW_pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import matplotlib as mpl\n",
    "\n",
    "# 1) Keep text as text in SVG; embed TrueType in PDF\n",
    "mpl.rcParams['svg.fonttype'] = 'none'   # SVG: do NOT convert text to paths\n",
    "mpl.rcParams['pdf.fonttype'] = 42       # PDF: use Type 42 (TrueType), not Type 3\n",
    "mpl.rcParams['ps.fonttype']  = 42\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = BoW_pca_X[:, 0]\n",
    "y = BoW_pca_X[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Add each filename as a label\n",
    "#for i, filename in enumerate(file_list):\n",
    "#    plt.text(x[i], y[i], filename, fontsize=13, ha='right', va='bottom')\n",
    "\n",
    "\n",
    "rename_models = {\n",
    "    \"gemini\": \"Gemini 2.0\",\n",
    "    \"claude\": \"Claude 3.7\",\n",
    "    \"chatgpt4o\": \"GPT-4o\",\n",
    "    \"chatgpto1\": \"o1\"\n",
    "}\n",
    "\n",
    "rename_tasks = {\n",
    "    \"Classification\": \"Classification\",\n",
    "    \"CCI\": \"Cellâ€“cell interaction\",\n",
    "    \"Pathway\": \"Pathway analysis\"\n",
    "}\n",
    "\n",
    "new_labels = []\n",
    "for filename in file_list:\n",
    "    base = filename.replace(\".txt\", \"\")\n",
    "    task, model = base.split(\"_\")\n",
    "    task_label = rename_tasks.get(task, task)\n",
    "    model_label = rename_models.get(model, model)\n",
    "    new_labels.append(f\"{task_label} ({model_label})\")\n",
    "\n",
    " \n",
    "    \n",
    "texts = []\n",
    "for i, label in enumerate(new_labels):\n",
    "    texts.append(plt.text(x[i], y[i], label, fontsize=12, ha='right', va='bottom'))\n",
    "\n",
    "# Automatically adjust positions to avoid overlap\n",
    "adjust_text(texts, x=x, y=y, arrowprops=dict(arrowstyle=\"->\", color='gray', lw=0.5))\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"PCA of BoW Vectors\", fontsize=20)\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=16)\n",
    "plt.ylabel(\"Principal Component 2\", fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# plt.savefig(\"pca_plot.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
