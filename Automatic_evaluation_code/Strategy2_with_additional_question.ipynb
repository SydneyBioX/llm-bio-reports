{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_df2U9b2o3Gs"
   },
   "source": [
    "# Baseline\n",
    "\n",
    "This is demo code for Baseline.\n",
    "\n",
    "Input files include text, image, csv table and the MC questions. The output are the MC answer.\n",
    "\n",
    "Input files include response from model 1 (chatGPT_response.txt) and the MC questions. The output are the MC answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aS_hJkFyUwe"
   },
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40580,
     "status": "ok",
     "timestamp": 1744762715210,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "ZdpMDYqiT9FT",
    "outputId": "65b45b36-c494-4c30-9c51-bc010fab1a81"
   },
   "outputs": [],
   "source": [
    "%pip install python-docx\n",
    "%pip install anthropic\n",
    "# !pip install mistralai\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11690,
     "status": "ok",
     "timestamp": 1744762726903,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "1TxsEQyCS3NO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "from openai import OpenAI\n",
    "# import google.generativeai as genai\n",
    "import anthropic\n",
    "# from mistralai import Mistral\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import docx\n",
    "import openpyxl\n",
    "import base64\n",
    "import chardet\n",
    "from typing import Union, List, Dict\n",
    "import tiktoken\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5dDChnDjJ6n"
   },
   "source": [
    "## File Reading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1744762726954,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "0HQvYJYSY4u6"
   },
   "outputs": [],
   "source": [
    "def file_to_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a DOCX, XLSX, CSV, JSON, RMD (R Markdown), or PY file\n",
    "    into a text string suitable for LLM input.\n",
    "\n",
    "    :param file_path: The path to the file to be converted.\n",
    "    :return: A single string containing the file’s textual contents.\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    if ext == \".docx\":\n",
    "        return _docx_to_text(file_path)\n",
    "    elif ext == \".xlsx\":\n",
    "        return _xlsx_to_text(file_path)\n",
    "    elif ext == \".csv\":\n",
    "        return _csv_to_text(file_path)\n",
    "    elif ext == \".json\":\n",
    "        return _json_to_text(file_path)\n",
    "    elif ext == \".rmd\":\n",
    "        return _rmarkdown_to_text(file_path)\n",
    "    elif ext == \".md\":\n",
    "        return _rmarkdown_to_text(file_path)\n",
    "    elif ext == \".py\":\n",
    "        return _python_to_text(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        return _txt_to_text(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "\n",
    "\n",
    "def _docx_to_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads a DOCX file and extracts all text paragraphs.\n",
    "    Requires 'python-docx' (pip install python-docx).\n",
    "    \"\"\"\n",
    "    if docx is None:\n",
    "        raise ImportError(\"Missing dependency 'python-docx'. Install via `pip install python-docx`.\")\n",
    "\n",
    "    doc = docx.Document(file_path)\n",
    "    paragraphs = [para.text for para in doc.paragraphs]\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "\n",
    "def _xlsx_to_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads an XLSX file and concatenates all cells as text.\n",
    "    Requires 'openpyxl' (pip install openpyxl).\n",
    "    \"\"\"\n",
    "    if openpyxl is None:\n",
    "        raise ImportError(\"Missing dependency 'openpyxl'. Install via `pip install openpyxl`.\")\n",
    "\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)\n",
    "    all_text = []\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        sheet = wb[sheet_name]\n",
    "        all_text.append(f\"--- Sheet: {sheet_name} ---\")\n",
    "        for row in sheet.iter_rows(values_only=True):\n",
    "            row_text = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "            all_text.append(\"\\t\".join(row_text))\n",
    "\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "\n",
    "def _csv_to_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads a CSV file line by line and returns its text representation.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            rows.append(\"\\t\".join(row))\n",
    "    return \"\\n\".join(rows)\n",
    "\n",
    "\n",
    "def _json_to_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads a JSON file and returns its pretty-printed JSON string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return json.dumps(data, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def _rmarkdown_to_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    R Markdown files are basically text files with embedded code.\n",
    "    We’ll just read the raw text for simplicity.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def _txt_to_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads a text file and returns its content as a string.\n",
    "    Automatically detects encoding to avoid decoding errors.\n",
    "    \"\"\"\n",
    "    #import chardet\n",
    "    try:\n",
    "        # Detect the file encoding\n",
    "        with open(file_path, 'rb') as file:\n",
    "            raw_data = file.read()\n",
    "            detected = chardet.detect(raw_data)\n",
    "            encoding = detected['encoding']\n",
    "\n",
    "        # Read the file with the detected encoding\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "def _python_to_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Python (.py) files are plain text. Just read the entire file content.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        code = f.read()\n",
    "    return code\n",
    "\n",
    "def encode_image(image_path: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Encode an image file to base64 and return its MIME type.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "    # Read and encode image\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Get MIME type based on file extension\n",
    "    _, extension = os.path.splitext(image_path)\n",
    "    extension = extension.lower()\n",
    "\n",
    "    mime_type = {\n",
    "        '.jpg': 'image/jpeg',\n",
    "        '.jpeg': 'image/jpeg',\n",
    "        '.png': 'image/png',\n",
    "        '.gif': 'image/gif',\n",
    "        '.webp': 'image/webp'\n",
    "    }.get(extension)\n",
    "\n",
    "    if not mime_type:\n",
    "        raise ValueError(f\"Unsupported image format: {extension}\")\n",
    "\n",
    "    return base64_data, mime_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjWA0AWljFv8"
   },
   "source": [
    "## Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1744762727297,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "8kumqmgLjFgQ"
   },
   "outputs": [],
   "source": [
    "def generate_MC_answer_openai(api_key: str, dir: str, rfile_path: str, images_folder_path: str, dfile_path: str, mc_txt: str, system_prompt: str = None, model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a result paragraph based on the given report and multiple-choice document using OpenAI's API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your OpenAI API key.\n",
    "        report_txt (str): The main report content to analyze.\n",
    "        mc_doc (str): The multiple-choice document content for the question.\n",
    "        system_prompt (str, optional): The system-level instruction for the AI. Defaults to a biological writing prompt.\n",
    "        model (str, optional): The OpenAI model to use. Defaults to \"gpt-4o\".\n",
    "\n",
    "    Returns:\n",
    "        str: The generated result paragraph.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Initialize OpenAI client\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # Use a default system prompt if none is provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are an expert scientist who has a strong background in both bioinformatics and biology.\"\n",
    "            \"I have provided you with a .txt file containing the outputs from my analysis, and in some cases, additional supporting materials such as plots.\"\n",
    "            \"These files are the core analytics you will need to understand to read the following multiple choice question.\"\n",
    "            \"When answering the multiple choice question, please select one option only.\"\n",
    "            \"Please be concise without including any additional information. For example if you think option A is the correct answer, then please just output A.\"\n",
    "        )\n",
    "\n",
    "    # Prepare the input message\n",
    "    user_prompt=[]\n",
    "\n",
    "    user_prompt.append(\n",
    "       \"Please read the following .txt file and any supporting materials and use the information to answer the multiple choice question below.\\n Please provide only the letter of the correct option (A, B, C, D, or E). Do not include the answer text, explanations, or any other information.\"\n",
    "        )\n",
    "\n",
    "    if pd.notna(rfile_path):\n",
    "        report_txt = file_to_text(os.path.join(dir,rfile_path))\n",
    "        user_prompt.append(\"Txt file:\\n\" + report_txt)\n",
    "\n",
    "    # if pd.notna(dfile_path):\n",
    "    #     data_txt = file_to_text(os.path.join(dir,dfile_path))\n",
    "    #     user_prompt.append(\"I have a csv file:\" + \"\\n\" + data_txt)\n",
    "\n",
    "    if pd.notna(dfile_path):\n",
    "        # Split dfile_path into individual file paths\n",
    "        file_paths = dfile_path.split(\"\\n\")\n",
    "        if len(file_paths)>1:\n",
    "            user_prompt.append(\"CSV files:\\n\")\n",
    "        else:\n",
    "            user_prompt.append(\"CSV file:\\n\")\n",
    "        for file_path in file_paths:\n",
    "            # Process each file path\n",
    "            full_path = os.path.join(dir, file_path.strip())  # Strip any extra spaces or newline characters\n",
    "            if os.path.exists(full_path):  # Ensure the file exists before reading\n",
    "                data_txt = file_to_text(full_path)  # Convert the file to text\n",
    "                user_prompt.append(data_txt+\"\\n\")\n",
    "            else:\n",
    "                print(f\"File not found: {full_path}\")\n",
    "\n",
    "\n",
    "    if pd.notna(images_folder_path):\n",
    "        image_path_list = []\n",
    "        image_dir=os.path.join(dir,images_folder_path,\"figure-markdown_strict\")\n",
    "        for file_name in os.listdir(image_dir):\n",
    "            if file_name.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
    "                image_path_list.append(os.path.join(image_dir, file_name))\n",
    "        image_file_list = [encode_image(item) for item in image_path_list]\n",
    "        for image_file,mime_type in image_file_list:\n",
    "            user_prompt.append(\"Image file:\\n\")\n",
    "            user_prompt.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:{mime_type};base64,{image_file}\",\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    user_prompt.append(\"Multiple choice question: \\n\"\n",
    "                       + mc_txt )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "    ]\n",
    "\n",
    "    # print(\"Number of tokens:\", count_tokens(messages,model=model))\n",
    "\n",
    "    #Generate the completion\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            # temperature=0.1\n",
    "        )\n",
    "        mcq_answer = chat_completion.choices[0].message.content\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": mcq_answer})\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": \"Please indicate whether the information needed to answer the multiple-choice question can be found in the document. Respond with 'Yes' or 'No' only. Do not provide explanations or additional details.\"})\n",
    "        response_yes_no = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages  # Now includes Q1, A1, and Q2\n",
    "        )\n",
    "        yes_no_answer = response_yes_no.choices[0].message.content\n",
    "        return mcq_answer, yes_no_answer\n",
    "    except Exception as e:\n",
    "        # raise RuntimeError(f\"Error generating result paragraph: {e}\")\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "            # Return a default error string\n",
    "        return \"error\", \"error\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1744762727449,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "i3O0IeMgiONu"
   },
   "outputs": [],
   "source": [
    "def generate_MC_answer_google_gemini(api_key: str, dir: str, rfile_path: str, images_folder_path: str, dfile_path: str, mc_txt: str, system_prompt: str = None, model: str = \"gemini-2.0-flash-exp\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a result paragraph based on the given report and multiple-choice document using Google's Generative AI API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your Google Generative AI API key.\n",
    "        report_txt (str): The main report content to analyze.\n",
    "        mc_doc (str): The multiple-choice document content for the question.\n",
    "        system_prompt (str, optional): The system-level instruction for the AI. Defaults to a biological writing prompt.\n",
    "        model (str, optional): The Google Generative AI model to use. Defaults to \"models/chat-bison-001\".\n",
    "\n",
    "    Returns:\n",
    "        str: The generated result paragraph.\n",
    "    \"\"\"\n",
    "\n",
    "    # Configure the API key\n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    # Use a default system prompt if none is provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are an expert scientist who has a strong background in both bioinformatics and biology.\"\n",
    "            \"I have provided you with a .txt file containing the outputs from my analysis, and in some cases, additional supporting materials such as plots.\"\n",
    "            \"These files are the core analytics you will need to understand to read the following multiple choice question.\"\n",
    "            \"When answering the multiple choice question, please select one option only.\"\n",
    "            \"Please be concise without including any additional information. For example if you think option A is the correct answer, then please just output A.\"\n",
    "        )\n",
    "\n",
    "    # Prepare the input message\n",
    "    user_prompt=[]\n",
    "\n",
    "    user_prompt.append(\n",
    "       \"Please read the following .txt file and any supporting materials and use the information to answer the multiple choice question below.\\n Please provide only the letter of the correct option (A, B, C, D, or E). Do not include the answer text, explanations, or any other information.\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    if pd.notna(rfile_path):\n",
    "        report_txt = file_to_text(os.path.join(dir,rfile_path))\n",
    "        user_prompt.append(\"Txt file:\\n\" + report_txt)\n",
    "\n",
    "    # if pd.notna(dfile_path):\n",
    "    #     data_txt = file_to_text(os.path.join(dir,dfile_path))\n",
    "    #     user_prompt.append(\"I have a csv file:\" + \"\\n\" + data_txt)\n",
    "\n",
    "    if pd.notna(dfile_path):\n",
    "        # Split dfile_path into individual file paths\n",
    "        file_paths = dfile_path.split(\"\\n\")\n",
    "        if len(file_paths)>1:\n",
    "            user_prompt.append(\"CSV files:\\n\")\n",
    "        else:\n",
    "            user_prompt.append(\"CSV file:\\n\")\n",
    "        for file_path in file_paths:\n",
    "            # Process each file path\n",
    "            full_path = os.path.join(dir, file_path.strip())  # Strip any extra spaces or newline characters\n",
    "            if os.path.exists(full_path):  # Ensure the file exists before reading\n",
    "                data_txt = file_to_text(full_path)  # Convert the file to text\n",
    "                user_prompt.append(data_txt+\"\\n\")\n",
    "            else:\n",
    "                print(f\"File not found: {full_path}\")\n",
    "\n",
    "\n",
    "    if pd.notna(images_folder_path):\n",
    "        image_path_list = []\n",
    "        image_dir=os.path.join(dir,images_folder_path,\"figure-markdown_strict\")\n",
    "        for file_name in os.listdir(image_dir):\n",
    "            if file_name.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
    "                image_path_list.append(os.path.join(image_dir, file_name))\n",
    "        image_file_list = [encode_image(item) for item in image_path_list]\n",
    "        for image_file,mime_type in image_file_list:\n",
    "            user_prompt.append(\"Image file:\\n\")\n",
    "            user_prompt.append(\n",
    "                # {'mime_type': mime_type, 'data': image_file}\n",
    "                # image_file\n",
    "                types.Part.from_bytes(data=image_file, mime_type=mime_type)\n",
    "            )\n",
    "\n",
    "    user_prompt.append(\"Multiple choice question: \\n\"\n",
    "                       + mc_txt )\n",
    "\n",
    "    # messages = [\n",
    "    #     {\"role\": \"user\", \"parts\": f\"{user_prompt}\"},\n",
    "    # ]\n",
    "\n",
    "\n",
    "        # Create the model\n",
    "    # generation_config = {\n",
    "    #   # \"temperature\": 0.1,\n",
    "    #   \"top_p\": 0.95,\n",
    "    #   \"top_k\": 40,\n",
    "    #   \"max_output_tokens\": 8192,\n",
    "    #   \"response_mime_type\": \"text/plain\",\n",
    "    # }\n",
    "\n",
    "    try:\n",
    "        # chat_session = model.start_chat()\n",
    "        # chat_session = model.start_chat(history=messages)\n",
    "        # Send an additional message if needed (optional)\n",
    "        # response = chat_session.send_message(messages)\n",
    "        chat = client.chats.create(model=model,\n",
    "                                   config=types.GenerateContentConfig(\n",
    "                # generation_config=generation_config,\n",
    "                # temperature= 0.5,\n",
    "                response_mime_type=\"text/plain\",\n",
    "                system_instruction=system_prompt,\n",
    "            ))\n",
    "        response_mcq =  chat.send_message(\n",
    "            user_prompt\n",
    "        )\n",
    "\n",
    "        response_yes_no = chat.send_message(\"Does the report contain the information necessary to answer the multiple-choice question? Please provide only 'Yes' or 'No'. Do not include explanations or additional details.\")\n",
    "\n",
    "        return response_mcq.text.strip(), response_yes_no.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # raise RuntimeError(f\"Error generating result paragraph with Generative AI: {e}\")\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "            # Return a default error string\n",
    "        return \"error\", \"error\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1744762727509,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "1wG3Z0Il-vRU"
   },
   "outputs": [],
   "source": [
    "def generate_MC_answer_claude(api_key: str, dir: str, rfile_path: str, images_folder_path: str, dfile_path: str, mc_txt: str, system_prompt: str = None, model: str = \"claude-3-5-sonnet-20241022\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice answer based on the given report and document using Claude's API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your Claude API key.\n",
    "        report_txt (str): The main report content to analyze.\n",
    "        mc_doc (str): The multiple-choice document content for the question.\n",
    "        system_prompt (str, optional): The system-level instruction for the AI. Defaults to a biological writing prompt.\n",
    "        model (str, optional): The Claude model to use. Defaults to \"claude-3-5-sonnet-20241022\".\n",
    "\n",
    "    Returns:\n",
    "        str: The multiple-choice answer generated by Claude.\n",
    "    \"\"\"\n",
    "    # Initialize the Claude API client\n",
    "    client = anthropic.Anthropic(\n",
    "        api_key=api_key\n",
    "    )\n",
    "\n",
    "    # Use a default system prompt if none is provided\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are an expert scientist who has a strong background in both bioinformatics and biology.\"\n",
    "            \"I have provided you with a .txt file containing the outputs from my analysis, and in some cases, additional supporting materials such as plots.\"\n",
    "            \"These files are the core analytics you will need to understand to read the following multiple choice question.\"\n",
    "            \"When answering the multiple choice question, please select one option only.\"\n",
    "            \"Please be concise without including any additional information. For example if you think option A is the correct answer, then please just output A.\"\n",
    "        )\n",
    "\n",
    "    # Prepare the input message\n",
    "    user_prompt=[]\n",
    "    user_prompt.append(\n",
    "      {\"type\": \"text\", \"text\": \"Please read the following .txt file and any supporting materials and use the information to answer the multiple choice question below.\\n Please provide only the letter of the correct option (A, B, C, D, or E). Do not include the answer text, explanations, or any other information.\"\n",
    "      })\n",
    "\n",
    "\n",
    "    if pd.notna(rfile_path):\n",
    "        report_txt = file_to_text(os.path.join(dir,rfile_path))\n",
    "        user_prompt.append({\"type\": \"text\", \"text\":\"Txt file:\\n\" + report_txt})\n",
    "\n",
    "    # if pd.notna(dfile_path):\n",
    "    #     data_txt = file_to_text(os.path.join(dir,dfile_path))\n",
    "    #     user_prompt.append(\"I have a csv file:\" + \"\\n\" + data_txt)\n",
    "\n",
    "    if pd.notna(dfile_path):\n",
    "        # Split dfile_path into individual file paths\n",
    "        file_paths = dfile_path.split(\"\\n\")\n",
    "        if len(file_paths)>1:\n",
    "            user_prompt.append({\"type\": \"text\", \"text\":\"CSV files:\\n\"})\n",
    "        else:\n",
    "            user_prompt.append({\"type\": \"text\", \"text\":\"CSV file:\\n\"})\n",
    "        for file_path in file_paths:\n",
    "            # Process each file path\n",
    "            full_path = os.path.join(dir, file_path.strip())  # Strip any extra spaces or newline characters\n",
    "            if os.path.exists(full_path):  # Ensure the file exists before reading\n",
    "                data_txt = file_to_text(full_path)  # Convert the file to text\n",
    "                user_prompt.append({\"type\": \"text\", \"text\": data_txt+\"\\n\"})\n",
    "            else:\n",
    "                print(f\"File not found: {full_path}\")\n",
    "\n",
    "\n",
    "    if pd.notna(images_folder_path):\n",
    "        image_path_list = []\n",
    "        image_dir=os.path.join(dir,images_folder_path,\"figure-markdown_strict\")\n",
    "        for file_name in os.listdir(image_dir):\n",
    "            if file_name.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\")):\n",
    "                image_path_list.append(os.path.join(image_dir, file_name))\n",
    "        image_file_list = [encode_image(item) for item in image_path_list]\n",
    "        for image_file,mime_type in image_file_list:\n",
    "            user_prompt.append( \"Image file:\\n\")\n",
    "            user_prompt.append(\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": mime_type,  # e.g., \"image/jpeg\"\n",
    "                    \"data\": image_file\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    user_prompt.append({\"type\": \"text\", \"text\":\"Multiple choice question: \\n\"\n",
    "                       + mc_txt} )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Call the Claude API to generate the response\n",
    "    try:\n",
    "        response_mcq = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=8192,\n",
    "           # temperature=0.1,\n",
    "            system=system_prompt,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        mcq_answer = response_mcq.content[0].text.strip()\n",
    "\n",
    "     # Append the response to maintain conversation history\n",
    "        messages.append({\"role\": \"assistant\", \"content\": mcq_answer})\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\":\"Does the report contain the information necessary to answer the multiple-choice question? Please provide only 'Yes' or 'No'. Do not include explanations or additional details.\"})\n",
    "\n",
    "        response_yes_no = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            system=system_prompt,\n",
    "            messages=messages  # Now includes Q1, A1, and Q2\n",
    "        )\n",
    "        yes_no_answer = response_yes_no.content[0].text.strip()\n",
    "\n",
    "        return mcq_answer, yes_no_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        # raise RuntimeError(f\"Error generating multiple-choice answer with Claude: {e}\")\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "            # Return a default error string\n",
    "        return \"error\", \"error\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HxN-IxvyZkb"
   },
   "source": [
    "## Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21971,
     "status": "ok",
     "timestamp": 1744762749572,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "XGWHA_iY7MTa",
    "outputId": "127d08b2-a989-4512-ecf0-0259915ce960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG_egwC9yeS9"
   },
   "source": [
    "## Read Prompt Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1744762750564,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "TgG4tJV99Ecr",
    "outputId": "51466fb1-d426-4992-977f-ab0944a4168e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Authors          Google Folder              CaseStudy_ID  \\\n",
      "0  EXAMPLE 1  Bioinformatics_method  Dataset_method_increment   \n",
      "1  EXAMPLE 2  Bioinformatics_method  Dataset_method_increment   \n",
      "2  EXAMPLE 3  Bioinformatics_method  Dataset_method_increment   \n",
      "3         DK                Pathway          Kidney_pathway_1   \n",
      "4         DK                     DE               Kidney_DE_1   \n",
      "5         DK                Pathway          Kidney_pathway_2   \n",
      "6         YC                    CCI               Covid_CCI_1   \n",
      "7         YC                    CCI               Covid_CCI_1   \n",
      "8         YC                    CCI               Covid_CCI_1   \n",
      "\n",
      "                   Sample_ID LLM task updated?    Input type  \\\n",
      "0  Dataset_method_incrementA                No         Graph   \n",
      "1   Dataset_method_increment                No          Data   \n",
      "2  Dataset_method_incrementB                No   Code + Data   \n",
      "3          Kidney_pathway_1B                No  Code + Graph   \n",
      "4                Kidney_DE_1                No   Code + Data   \n",
      "5           Kidney_pathway_2                No   Code + Data   \n",
      "6               Covid_CCI_1A                No  Code + Graph   \n",
      "7               Covid_CCI_1B                No   Code + Data   \n",
      "8               Covid_CCI_1C                No         Graph   \n",
      "\n",
      "                 RMD_Code_input_ID                     TXT_input_ID  \\\n",
      "0                              NaN                              NaN   \n",
      "1                              NaN                              NaN   \n",
      "2                              NaN                              NaN   \n",
      "3            Kidney_pathway_1B.Rmd            Kidney_pathway_1B.txt   \n",
      "4                  Kidney_DE_1.Rmd                  Kidney_DE_1.txt   \n",
      "5             Kidney_pathway_2.Rmd             Kidney_pathway_2.txt   \n",
      "6     CCI_chua_celltype_w_plot.Rmd     CCI_chua_celltype_w_plot.txt   \n",
      "7     CCI_chua_celltype_w_data.Rmd     CCI_chua_celltype_w_data.txt   \n",
      "8  CCI_chua_celltype_only_plot.Rmd  CCI_chua_celltype_only_plot.txt   \n",
      "\n",
      "            Graphics_input_Folder Data_input_ID  ... Task Difficulty  \\\n",
      "0                             NaN           NaN  ...          Simple   \n",
      "1                             NaN           NaN  ...             NaN   \n",
      "2                             NaN           NaN  ...             NaN   \n",
      "3          Kidney_pathway_1_files           NaN  ...          Simple   \n",
      "4                             NaN           NaN  ...          Simple   \n",
      "5                             NaN           NaN  ...          Simple   \n",
      "6  CCI_chua_celltype_w_plot_files           NaN  ...         Complex   \n",
      "7                             NaN           NaN  ...         Complex   \n",
      "8  CCI_chua_celltype_w_plot_files           NaN  ...         Complex   \n",
      "\n",
      "  Task Difficulty (evaluated by Fei) (# clues given - score by human)   \\\n",
      "0                             Simple                                 0   \n",
      "1                             Simple                                 1   \n",
      "2                             Simple                                 2   \n",
      "3                            Median                                  2   \n",
      "4                             Simple                                 2   \n",
      "5                            Median                                  2   \n",
      "6                            Median                                  1   \n",
      "7                            Median                                  1   \n",
      "8                            Median                                  1   \n",
      "\n",
      "  Task (this should match what's in your .rmd file)  MC1 ID (easy)  \\\n",
      "0  What are the pathways that are most associated...           1.0   \n",
      "1  These differentially expressed genes were iden...           4.0   \n",
      "2  These differentially expressed genes were iden...           7.0   \n",
      "3  This is a Gene Set Enrichment Analysis (GSEA) ...           1.0   \n",
      "4  This is a differential expression analysis of ...           1.0   \n",
      "5  This is a Gene Set Enrichment Analysis (GSEA) ...           2.0   \n",
      "6  This file performs cell cell communications fo...           1.0   \n",
      "7  This file performs cell cell communications fo...           1.0   \n",
      "8  This file performs cell cell communications fo...           1.0   \n",
      "\n",
      "   MC2 ID (harder) MC3 ID (harder)  \\\n",
      "0                2               3   \n",
      "1                5               6   \n",
      "2                8               9   \n",
      "3                -               -   \n",
      "4                -               -   \n",
      "5              NaN             NaN   \n",
      "6                2               3   \n",
      "7                2               3   \n",
      "8                2               3   \n",
      "\n",
      "   Analytical focus (Notes, bioinformatics category) Personal note  \\\n",
      "0  classic pathway analysis, I've filtered for si...           NaN   \n",
      "1  classic pathway analysis, I've filtered for si...           NaN   \n",
      "2  classic pathway analysis, I've filtered for si...           NaN   \n",
      "3                                            Pathway           NaN   \n",
      "4                                                 DE           NaN   \n",
      "5                                            Pathway           NaN   \n",
      "6                                                CCI           NaN   \n",
      "7                                                CCI           NaN   \n",
      "8                                                CCI           NaN   \n",
      "\n",
      "                               Lijia_note  \n",
      "0                                     NaN  \n",
      "1                                     NaN  \n",
      "2                                     NaN  \n",
      "3                                     NaN  \n",
      "4                                     NaN  \n",
      "5                                     NaN  \n",
      "6  Exceeded GPT-4o and claude token limit  \n",
      "7                                     NaN  \n",
      "8  Exceeded GPT-4o and claude token limit  \n",
      "\n",
      "[9 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "## Don't change this url\n",
    "url = 'https://docs.google.com/spreadsheets/d/XXXXXXXXXXXXXXXXXXX/export?format=csv&gid=XXXXXXXXXXXXXXXXXXX'\n",
    "case_df = pd.read_csv(url)\n",
    "print(case_df.head(9))\n",
    "df=case_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1744762751317,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "joSn8zulSEEN",
    "outputId": "cf7b0e46-9c3c-46f3-d54e-e17a118421f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many significant pathways are there after performing GSEA for both Biological Processes and Molecular Functions?\n",
      "A. 3\n",
      "B. 0\n",
      "C. 5\n",
      "D. 1\n",
      "E. None of the above\n",
      "Pathway\n",
      "Kidney_pathway_1\n"
     ]
    }
   ],
   "source": [
    "## Don't change this url\n",
    "url = 'https://docs.google.com/spreadsheets/d/XXXXXXXXXXXXXXXXXXX/export?format=csv&gid=XXXXXXXXXXXXXXXXXXX'\n",
    "mc_df = pd.read_csv(url)\n",
    "# print(mc_df.head(25))\n",
    "print(mc_df['Specific Question'][0])\n",
    "print(mc_df['Google Folder'][0])\n",
    "print(mc_df['CaseStudy_ID'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkJ_Gpy_kC7g"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1744762751327,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "IRzms8_2SE83"
   },
   "outputs": [],
   "source": [
    "OPENAI_api_key= \"sk-\"\n",
    "GEMINI_api_key= \"AI\"\n",
    "CLAUDE_api_key=\"sk-\"\n",
    "## root_dir is the only path that you need to modify by yourself.\n",
    "## You may find the shared Proj-LLM-Bioinfo-Interpretation2024 folder in the /content/drive/MyDrive,\n",
    "## so the dir path can be /content/drive/MyDrive/Proj-LLM-Bioinfo-Interpretation2024/Rmd_word_document/\n",
    "root_dir = \"/content/drive/MyDrive/Usyd/Proj-LLM-Bioinfo-Interpretation2024/Rmd_word_document/\"\n",
    "pattern = \"MC_*.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1744762751334,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "JNYefRRXUmXq",
    "outputId": "51380651-f58c-43d2-b5ad-483b8fb78e5a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Usyd/Proj-LLM-Bioinfo-Interpretation2024/Rmd_word_document/Bioinformatics_method/Dataset_method_increment'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### test\n",
    "os.path.join(root_dir, df['Google Folder'][1], df['CaseStudy_ID'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAI8aXYmlrpA"
   },
   "source": [
    "### Multiple Choice questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59JIHhkrSGsE"
   },
   "outputs": [],
   "source": [
    "columns_to_check = ['TXT_input_ID', 'Graphics_input_Folder', 'Data_input_ID']\n",
    "# index=np.where(mc_df[\"Authors\"] == \"YC\")[0]\n",
    "results = []\n",
    "for idx in range(len(mc_df)): #range(len(mc_df))[20:30] range(len(mc_df))\n",
    "    if pd.notna(mc_df['Google Folder'][idx]) and pd.notna(mc_df['CaseStudy_ID'][idx]):\n",
    "        subfolder = os.path.join(root_dir, mc_df['Google Folder'][idx].strip(), mc_df['CaseStudy_ID'][idx])\n",
    "        print(subfolder)\n",
    "        if os.path.exists(subfolder):\n",
    "            target_folder=mc_df['Google Folder'][idx].strip()\n",
    "            target_casestudy=mc_df['CaseStudy_ID'][idx]\n",
    "            mc_txt=mc_df['Specific Question'][idx]\n",
    "            print(\"Index: \",idx)\n",
    "            print(target_casestudy+\"_\"+str(mc_df['Question_ID'][idx]))\n",
    "            print(mc_txt)\n",
    "            filtered_df = case_df[case_df['CaseStudy_ID'] == target_casestudy]\n",
    "            for i in range(filtered_df.shape[0]): #\n",
    "                print(i)\n",
    "                row_entry=filtered_df.iloc[i]\n",
    "                print(row_entry[\"Sample_ID\"])\n",
    "                time.sleep(30)\n",
    "                gpt4o_mcq_answer, gpt4o_info_check =generate_MC_answer_openai(api_key=OPENAI_api_key, dir=subfolder,\n",
    "                                          rfile_path=row_entry[\"TXT_input_ID\"], images_folder_path=row_entry[\"Graphics_input_Folder\"],\n",
    "                                          dfile_path=row_entry[\"Data_input_ID\"], mc_txt=mc_txt, model = \"gpt-4o\")\n",
    "                gemini_mcq_answer, gemini_info_check =generate_MC_answer_google_gemini(api_key=GEMINI_api_key, dir=subfolder,\n",
    "                                          rfile_path=row_entry[\"TXT_input_ID\"], images_folder_path=row_entry[\"Graphics_input_Folder\"],\n",
    "                                          dfile_path=row_entry[\"Data_input_ID\"], mc_txt=mc_txt, model = \"gemini-2.0-flash\")\n",
    "                claude_mcq_answer, claude_info_check = generate_MC_answer_claude(api_key=CLAUDE_api_key, dir=subfolder,\n",
    "                                          rfile_path=row_entry[\"TXT_input_ID\"], images_folder_path=row_entry[\"Graphics_input_Folder\"],\n",
    "                                          dfile_path=row_entry[\"Data_input_ID\"], mc_txt=mc_txt, model = \"claude-3-7-sonnet-20250219\")\n",
    "                print(\"Sample ID: \"+row_entry['Sample_ID']+\", Question ID: \"+mc_df[\"CaseStudy_ID\"][idx]+\"_\"+str(mc_df[\"Question_ID\"][idx])+\", GPT4o answer:\"+gpt4o_mcq_answer+\", GPT4o info check:\"+gpt4o_info_check+\"\\n\")\n",
    "                print(\"Sample ID: \"+row_entry['Sample_ID']+\", Question ID: \"+mc_df[\"CaseStudy_ID\"][idx]+\"_\"+str(mc_df[\"Question_ID\"][idx])+\", Gemini answer:\"+gemini_mcq_answer+\", Gemini info check:\"+gemini_info_check+\"\\n\")\n",
    "                print(\"Sample ID: \"+row_entry['Sample_ID']+\", Question ID: \"+mc_df[\"CaseStudy_ID\"][idx]+\"_\"+str(mc_df[\"Question_ID\"][idx])+\", Claude answer:\"+claude_mcq_answer+\", Claude info check:\"+claude_info_check+\"\\n\")\n",
    "                model_answers = {\n",
    "                    \"gpt-4o\": gpt4o_mcq_answer,\n",
    "                    \"gemini-2.0-flash\": gemini_mcq_answer,\n",
    "                    \"claude-3-7-sonnet-20250219\": claude_mcq_answer#,\n",
    "                }\n",
    "\n",
    "                info_check={\n",
    "                        \"gpt-4o\": gpt4o_info_check,\n",
    "                        \"gemini-2.0-flash\": gemini_info_check,\n",
    "                        \"claude-3-7-sonnet-20250219\": claude_info_check\n",
    "                  }\n",
    "                for j in model_answers.keys():\n",
    "                    new_row = {\n",
    "                        \"CaseStudy_ID\": mc_df['CaseStudy_ID'][idx],\n",
    "                        \"Question_ID\":  mc_df['Question_ID'][idx],\n",
    "                        \"Answer\": mc_df['Answer'][idx],\n",
    "                        \"Sample_ID\": row_entry['Sample_ID'],\n",
    "                        \"Model_name\": j,\n",
    "                        \"Model_return\": model_answers[j],\n",
    "                        \"Info_check\": info_check[j]#,\n",
    "                    }\n",
    "                    selected_columns = [\"Authors\",\"Google Folder\", \"Data\", \"Task Category\"]  # Replace with actual column names \"Input type\",\n",
    "                    new_row.update(row_entry[selected_columns].to_dict())\n",
    "                    results.append(new_row)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744731406305,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "Cgn1PWwKhHBI",
    "outputId": "d9040b02-481b-465e-cd8f-e77eec1c9384"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 11)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1744731406344,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "hV_gNtH8nGdq",
    "outputId": "87eb7ea2-5638-446f-edc9-66fdeb201ec5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"result_df\",\n  \"rows\": 48,\n  \"fields\": [\n    {\n      \"column\": \"CaseStudy_ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Kidney_Pathway_4\",\n          \"Covid_CCI_11\",\n          \"Kidney_pathway_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"A\",\n          \"D\",\n          \"B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sample_ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"Kidney_pathway_1B\",\n          \"Kidney_Pathway_4B\",\n          \"Covid_CCI_11C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"claude-3-7-sonnet-20250219\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model_return\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Info_check\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Authors\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"DK\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Google Folder\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"CCI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Data\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Kidney\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Task Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Biological Inference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "result_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-390d9976-187c-4ba5-9010-05be27f9377d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaseStudy_ID</th>\n",
       "      <th>Question_ID</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>Model_name</th>\n",
       "      <th>Model_return</th>\n",
       "      <th>Info_check</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Google Folder</th>\n",
       "      <th>Data</th>\n",
       "      <th>Task Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kidney_pathway_1</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>Kidney_pathway_1B</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>B</td>\n",
       "      <td>Yes</td>\n",
       "      <td>DK</td>\n",
       "      <td>Pathway</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>Descriptive - information retrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kidney_pathway_1</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>Kidney_pathway_1B</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>A</td>\n",
       "      <td>Yes</td>\n",
       "      <td>DK</td>\n",
       "      <td>Pathway</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>Descriptive - information retrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covid_CCI_6</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>Covid_CCI_6A</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>C</td>\n",
       "      <td>Yes</td>\n",
       "      <td>YC</td>\n",
       "      <td>CCI</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Descriptive - information retrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Covid_CCI_6</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>Covid_CCI_6C</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>C</td>\n",
       "      <td>Yes</td>\n",
       "      <td>YC</td>\n",
       "      <td>CCI</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Descriptive - information retrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Covid_CCI_6</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>Covid_CCI_6A</td>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>B</td>\n",
       "      <td>Yes</td>\n",
       "      <td>YC</td>\n",
       "      <td>CCI</td>\n",
       "      <td>COVID</td>\n",
       "      <td>Descriptive - information retrival</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-390d9976-187c-4ba5-9010-05be27f9377d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-390d9976-187c-4ba5-9010-05be27f9377d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-390d9976-187c-4ba5-9010-05be27f9377d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-8d4ab468-a3ae-48f8-a75a-1f8fce5b5b1c\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d4ab468-a3ae-48f8-a75a-1f8fce5b5b1c')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-8d4ab468-a3ae-48f8-a75a-1f8fce5b5b1c button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       CaseStudy_ID  Question_ID Answer          Sample_ID  \\\n",
       "0  Kidney_pathway_1            2      B  Kidney_pathway_1B   \n",
       "1  Kidney_pathway_1            3      A  Kidney_pathway_1B   \n",
       "2       Covid_CCI_6            1      C       Covid_CCI_6A   \n",
       "3       Covid_CCI_6            1      C       Covid_CCI_6C   \n",
       "4       Covid_CCI_6            2      D       Covid_CCI_6A   \n",
       "\n",
       "                   Model_name Model_return Info_check Authors Google Folder  \\\n",
       "0  claude-3-7-sonnet-20250219            B        Yes      DK       Pathway   \n",
       "1  claude-3-7-sonnet-20250219            A        Yes      DK       Pathway   \n",
       "2  claude-3-7-sonnet-20250219            C        Yes      YC           CCI   \n",
       "3  claude-3-7-sonnet-20250219            C        Yes      YC           CCI   \n",
       "4  claude-3-7-sonnet-20250219            B        Yes      YC           CCI   \n",
       "\n",
       "     Data                       Task Category  \n",
       "0  Kidney  Descriptive - information retrival  \n",
       "1  Kidney  Descriptive - information retrival  \n",
       "2   COVID  Descriptive - information retrival  \n",
       "3   COVID  Descriptive - information retrival  \n",
       "4   COVID  Descriptive - information retrival  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744731406350,
     "user": {
      "displayName": "Lijia Yu",
      "userId": "00870759781493577221"
     },
     "user_tz": -600
    },
    "id": "8SIE4EvpakDl",
    "outputId": "d25f9252-3975-4829-d08e-e08901399463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Usyd/Proj-LLM-Bioinfo-Interpretation2024/MCQ_output/APRIL15_MCQ_result_strategy2_cleaned_2.csv\n"
     ]
    }
   ],
   "source": [
    "output_path=os.path.join(\"/content/drive/MyDrive/Usyd/Proj-LLM-Bioinfo-Interpretation2024/MCQ_output\",\"APRIL15_MCQ_result_strategy2_cleaned_2.csv\")\n",
    "print(output_path)\n",
    "# Check if the file exists\n",
    "file_exists = os.path.isfile(output_path)\n",
    "result_df[\"Model_return\"] = result_df[\"Model_return\"].str.strip().str.replace(r\"\\.$\", \"\", regex=True)\n",
    "# Save data: Append if file exists, otherwise create a new one\n",
    "result_df.to_csv(output_path, mode='a', index=False, header=not file_exists)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bidcell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
